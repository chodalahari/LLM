# -*- coding: utf-8 -*-
"""inputpipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R6NXrTDP84KAPZMKY1h1UEHElCplOd05
"""

import torch

import torch.nn as nn



# Fix seed so output is stable but different from your friend's

torch.manual_seed(7)



sentence = "Life feels slow"



tokens = sentence.split()



print("Step 1: Tokens")

print(tokens)



vocab = {token: idx for idx, token in enumerate(tokens)}

token_ids = torch.tensor([vocab[token] for token in tokens])



print("\nStep 2: Token IDs")

for token, idx in vocab.items():

  print(f"{token} â†’ {idx}")



embedding_dim = 6



token_embedding_layer = nn.Embedding(len(vocab), embedding_dim)

token_embeddings = token_embedding_layer(token_ids)



print("\nStep 3: Token Embeddings")

for token, emb in zip(tokens, token_embeddings):

  print(f"\n{token} token embedding:\n{emb}")



max_len = len(tokens)

position_ids = torch.arange(max_len)



positional_embedding_layer = nn.Embedding(max_len, embedding_dim)

positional_embeddings = positional_embedding_layer(position_ids)



print("\nStep 4: Positional Embeddings")

for i, pos_emb in enumerate(positional_embeddings):

  print(f"\nPosition {i} embedding:\n{pos_emb}")



final_input_vectors = token_embeddings + positional_embeddings



print("\nStep 5: Final Input Vectors (Token + Position)")

for token, final_vec in zip(tokens, final_input_vectors):

  print(f"\nFinal input vector for '{token}':\n{final_vec}")