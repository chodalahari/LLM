# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12AKr2HKWGG-In-NYbRlG7G8EuFts-YrZ
"""

text = "I love Python programming"
tokens = text.split()

print(tokens)

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

text = "Hugging Face makes NLP easy"
tokens = tokenizer.tokenize(text)

print(tokens)

import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Download required data (run once)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')

text = "Natural Language Processing is very interesting and useful"

# Tokenization
tokens = word_tokenize(text)

# Stopword removal
stop_words = set(stopwords.words('english'))
filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

print(filtered_tokens)

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

text = "Subword tokenization is powerful"
tokens = tokenizer.tokenize(text)

print(tokens)

from nltk.tokenize import word_tokenize

text = "NLP"
tokens = list(text)

print(tokens)

import nltk
from nltk.tokenize import sent_tokenize

nltk.download('punkt')   # run once

text = "I love NLP. It is very useful."
sentences = sent_tokenize(text)

print(sentences)

from nltk.tokenize import word_tokenize

text = "I love NLP."
words = word_tokenize(text)

print(words)

from sklearn.preprocessing import OneHotEncoder
import numpy as np

data = np.array([["cat"], ["dog"], ["fish"]])

encoder = OneHotEncoder(sparse_output=False)
encoded = encoder.fit_transform(data)

print(encoded)

from sklearn.feature_extraction.text import CountVectorizer

documents = ["I love NLP", "I love Machine Learning"]

vectorizer = CountVectorizer()
bow = vectorizer.fit_transform(documents)

print("Vocabulary:", vectorizer.get_feature_names_out())
print("BoW Matrix:\n", bow.toarray())

import gensim.downloader as api

# Load pretrained GloVe model
glove = api.load("glove-wiki-gigaword-50")

# User input
word = input("Enter a word: ").lower()

if word in glove:
    print("GloVe Embedding:")
    print(glove[word])
else:
    print("Word not found in GloVe vocabulary")

!pip install gensim

from transformers import BertTokenizer, BertModel
import torch

# Load pretrained BERT
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")

text = "I love Natural Language Processing"

# Tokenize
inputs = tokenizer(text, return_tensors="pt")

# Get embeddings
with torch.no_grad():
    outputs = model(**inputs)

# Last hidden state (token embeddings)
token_embeddings = outputs.last_hidden_state

print(token_embeddings.shape)

from gensim.models import Word2Vec

sentences = [
    ["i", "love", "nlp"],
    ["nlp", "is", "powerful"],
    ["i", "enjoy", "machine", "learning"]
]

model = Word2Vec(
    sentences,
    vector_size=100,
    window=5,
    min_count=1,
    sg=1   # 1 = Skip-Gram, 0 = CBOW
)
vector = model.wv["nlp"]
print(vector.shape)

from gensim.models import Word2Vec

# Read input sentence from the user
sentence = input("Enter text for Word2Vec: ")

# Simple tokenization using split and lowercase
word_tokens = sentence.lower().split()

# Train Word2Vec model on the token list
w2v_model = Word2Vec(
    sentences=[word_tokens],
    vector_size=30,
    window=2,
    min_count=1,
    workers=1,
    sg=0   # CBOW model
)

# Display vector representation for each word
print("Word vectors generated using Word2Vec:")
for token in word_tokens:
    print(f"{token} :", w2v_model.wv[token])

import sys
if 'gensim' not in sys.modules:
    !pip install gensim

import gensim.downloader as gensim_api

# Load pretrained GloVe vectors (50-dimensional)
glove_model = gensim_api.load("glove-wiki-gigaword-50")

# Accept a word from the user
input_word = input("Enter a single word: ").strip().lower()

# Retrieve and display embedding if available
if input_word in glove_model.key_to_index:
    print("Vector representation using GloVe:")
    print(glove_model[input_word])
else:
    print("The given word is not available in the GloVe vocabulary.")

import torch
from transformers import AutoTokenizer, AutoModel

# Load pretrained BERT tokenizer and model
bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
bert_model = AutoModel.from_pretrained("bert-base-uncased")

# Read input text from the user
sentence = input("Enter a text input: ")

# Prepare input for BERT
encoded_input = bert_tokenizer(sentence, padding=True, truncation=True, return_tensors="pt")

# Disable gradient calculation for inference
with torch.no_grad():
    model_output = bert_model(**encoded_input)

# Extract the embedding corresponding to [CLS] token
sentence_embedding = model_output.last_hidden_state[0][0]

# Display results
print("Embedding Dimension:", sentence_embedding.shape)
print("BERT Sentence Embedding:")
print(sentence_embedding)

